{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "Lecture - 12 slides for gain and entropy equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report\n",
    "\n",
    "In this project we worked with two datasets iris and spambase. The datasets were loaded into a variable using genfromtxt then normalized and radomized. The datasets were split into 10:90 testing and trainign sets respectively. The decision trees were initialized and for 10 folds the accuracy of its predictions were calculated and averaged. Instead, of growing full trees, a stopping mechanism was implemeted that once all the labels from the datasets were the same, the tree woud stop as that decsion would be unanimous throughout. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from the file using numpy genfromtxt with delimiter ','\n",
    "def load_csv(file):\n",
    "    X = np.genfromtxt(file,delimiter=\",\",dtype=str)\n",
    "    return (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the entire dataset prior to learning using min-max normalization \n",
    "def normalize(matrix):\n",
    "#   transfer the data metrix to np array in float type.\n",
    "    a = np.array(matrix, dtype=float)\n",
    "    #print(\"normalizing the entire dataset:\")\n",
    "    #print(a)\n",
    "    #print(\"Before normalizing\")\n",
    "#   apply the normalization along the 0 axis of a using the formula: (x - x_min)/(x_max - x_min)\n",
    "    a = (a - a.min(axis=0))/(a.max(axis=0) - a.min(axis=0))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to randomly shuffle the array using the numpy.random.shuffle()\n",
    "def random_numpy_array(ar):\n",
    "    np.random.shuffle(ar)\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data and generate the training labels,training features, test labels and test training\n",
    "def generate_set(X):\n",
    "    #print(X.shape[0])\n",
    "    \n",
    "#     store the label X[:,-1] to Y  So what that line did is sliced the array, taking all rows (:) but keeping the second column (-1)\n",
    "    Y = X[:,-1]\n",
    "    \n",
    "#     reshape y to (Y's length, 1)\n",
    "#     store it to j\n",
    "    j = Y.reshape(len(Y), 1)\n",
    "    #print(\"J is\",j)\n",
    "    \n",
    "#     create the new_X which exclude the label X[:,:-1]\n",
    "    X = X[:, :-1]\n",
    "    \n",
    "#     normalize the data step\n",
    "#     using our implemented function normalize()\n",
    "    X = normalize(X)\n",
    "    \n",
    "#     add the label back to the normiazlied X\n",
    "#     using np.concatenate along axis=1\n",
    "    X = np.concatenate((X, j), axis = 1)\n",
    "    \n",
    "#     store the size of rows of the normalized X with labels\n",
    "    row_size = X.shape[1]\n",
    "    \n",
    "#     use the 10% of the data to be the test set.\n",
    "#     store the number of testing data\n",
    "#     set the ending index to be the number of testing data\n",
    "    end = round((X.shape[0]) * 0.1)\n",
    "    \n",
    "#     set the starting idex to be 0\n",
    "    start = 0\n",
    "\n",
    "#     create a list that store all features of the testing data\n",
    "    test_f = []\n",
    "    \n",
    "#     create a list that store all labels of the testing data\n",
    "    test_l = []\n",
    "    \n",
    "#     create a list that store all features of the training data\n",
    "    train_f = []\n",
    "    \n",
    "#     create a list that store all labels of the training data\n",
    "    train_l = []\n",
    "    \n",
    "#     10-fold cross-validation:\n",
    "    for i in range(10):\n",
    "#         store the test set for corss-validation using X[start:end,:]\n",
    "        X_test = X[start:end, :]\n",
    "\n",
    "#         get training data before the testing data X[:start, :]\n",
    "        X_train_b = X[:start, :]\n",
    "    \n",
    "#         get training data after the testing data X[:start, :]\n",
    "        X_train_a = X[end:, :]\n",
    "\n",
    "#         form the new training set using np.concatenate\n",
    "        X_train = np.concatenate((X_train_b, X_train_a), axis = 0)\n",
    "    \n",
    "#         get the testing set labels\n",
    "        label_test = X_test[:,-1]\n",
    "        \n",
    "#         flattent the labels\n",
    "        label_test.flatten()\n",
    "    \n",
    "#         get the training set labels\n",
    "        label_train = X_train[:,-1]\n",
    "    \n",
    "#         flattent the labels\n",
    "        label_train.flatten()\n",
    "    \n",
    "#         create the test set exclude the labels\n",
    "        X_test = X_test[:, :-1]\n",
    "    \n",
    "#         same for the training set\n",
    "        X_train = X_train[:, :-1]\n",
    "\n",
    "#         append test data of this fold to the list\n",
    "        \n",
    "        test_f.append(X_test)\n",
    "    \n",
    "#         append test lables of this fold to the list\n",
    "        test_l.append(label_test)\n",
    "    \n",
    "#         do the same for the training set\n",
    "        train_f.append(X_train)\n",
    "        train_l.append(label_train)\n",
    "\n",
    "#         update the index pointer\n",
    "        start = end\n",
    "        end = end + round((X.shape[0]) * 0.1)\n",
    "\n",
    "#     return the fold list that contain data and label for both training and testing set.\n",
    "    return test_f, test_l, train_f, train_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a dictionary where the key is the class label and values are the features which belong to that class.\n",
    "def build_dict_of_attributes_with_class_values(X,y):\n",
    "#     init a dict for attributes\n",
    "    dict_att = {}\n",
    "    \n",
    "#     init feature list.\n",
    "    features =[]\n",
    "    \n",
    "#     for each feature in the dataset\n",
    "    for i in range(X.shape[1]):\n",
    "#     find all the value correspond to this feature\n",
    "        values = X[:,i]\n",
    "    \n",
    "#     init an attribute list\n",
    "        attribute = []\n",
    "    \n",
    "#     init the counter to 0\n",
    "        count = 0\n",
    "    \n",
    "#         for each value in the \"all the value correspond to this feature\"\n",
    "        for x in values: \n",
    "#             init a empty list that store the attribute value\n",
    "            att_value = []\n",
    "    \n",
    "#             append the this value to the list\n",
    "            att_value.append(x)\n",
    "    \n",
    "#             append the label of this value to the list\n",
    "            att_value.append(y[count])\n",
    "    \n",
    "#             append this list to the attribute list.\n",
    "            attribute.append(att_value)\n",
    "    \n",
    "#             increase the counter\n",
    "            count += 1\n",
    "    \n",
    "#         add this attribute list to the dict according to the feature index\n",
    "        dict_att[i] = attribute\n",
    "    \n",
    "#         append the feature indx to the feature list.\n",
    "        features.append(i)\n",
    "    \n",
    "#     return the dict and feature list.\n",
    "    return dict_att, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative Dichotomiser 3 entropy calculation\n",
    "def entropy(y):\n",
    "#     init a class frequence dict\n",
    "    class_freq ={}\n",
    "    \n",
    "#     init the attribute entropy to 0\n",
    "    entropy = 0\n",
    "    \n",
    "#     for each label in y:\n",
    "    for i in y:\n",
    "#         this is label is already in the dict, we increase its feq\n",
    "        if i in class_freq:\n",
    "            x = class_freq[i]\n",
    "            class_freq[i] = x+1\n",
    "#          else, we set the freq to 1\n",
    "        else: \n",
    "            class_freq[i] = 1\n",
    "#     calculate the cumulate entropy using the formula.\n",
    "    for x in class_freq.values():\n",
    "        entropy += -(x/len(y))*math.log(x/len(y), 2)\n",
    "    \n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class node and explanation is self explaination\n",
    "class Node(object):\n",
    "#     init the node with val,lchild,rchild,thea and leaf.\n",
    "    def __init__(self, val, lchild, rchild,thea,leaf):\n",
    "        self.root_value = val\n",
    "        self.root_left = lchild\n",
    "        self.root_right = rchild\n",
    "        self.theta = thea\n",
    "        self.leaf = leaf\n",
    "\n",
    "#     method to identify if the node is leaf\n",
    "    def is_leaf(self): \n",
    "        return self.leaf\n",
    "\n",
    "#     method to return threshold value\n",
    "    def ret_thetha(self):\n",
    "        return self.theta\n",
    "    \n",
    "#     method return root value\n",
    "    def ret_root_value(self):\n",
    "        return self.root_value\n",
    "    \n",
    "#     method return left tree\n",
    "    def ret_llist(self):\n",
    "        return self.root_left\n",
    "\n",
    "#     method return right tree\n",
    "    def ret_rlist(self):\n",
    "        return self.root_right\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(%r, %r, %r, %r)\" %(self.root_value,self.root_left,self.root_right,self.theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree object\n",
    "class DecisionTree(object):\n",
    "#     init a variable called fea_list\n",
    "    fea_list = []\n",
    "    \n",
    "#     init the Dtree by setting the root node to None\n",
    "    def __init__(self):\n",
    "        self.root_node = None\n",
    "\n",
    "    #method to return the major class value using Counter() and .most_common()\n",
    "    def cal_major_class_values(self,class_values):\n",
    "        freq = Counter(class_values).most_common (1)\n",
    "        return freq[0][0]\n",
    "\n",
    "    #method to calculate best threshold value for each feature\n",
    "    def cal_best_theta_value(self,ke,attri_list):\n",
    "#         init a list for data\n",
    "        data = []\n",
    "    \n",
    "#         init a list for class labes\n",
    "        class_labels = []\n",
    "        \n",
    "#         for each attribute in the attri_list\n",
    "        for i in attri_list:\n",
    "#             append the data\n",
    "            data.append(float(i[0]))\n",
    "    \n",
    "#             append the feature value.\n",
    "            class_labels.append(i[1])\n",
    "    \n",
    "#         calculate the entropy of those feaure values\n",
    "        etr = entropy(class_labels)\n",
    "#         init the max info gain = 0\n",
    "        max_info_gain = 0\n",
    "    \n",
    "#         init theta=0\n",
    "        theta = 0\n",
    "    \n",
    "#         init a list that store the best index on the left\n",
    "        best_l = []\n",
    "    \n",
    "#         init a list that store the best index on the right\n",
    "        best_r = []\n",
    "    \n",
    "#         init a list that store class labels after split\n",
    "        split_labels =[]\n",
    "    \n",
    "#         sort the data\n",
    "        data.sort()\n",
    "    \n",
    "#         for each index of data:\n",
    "        for i in range(len(data)-1):\n",
    "    \n",
    "#             calculate the current theta using data[i]+data[i+1])/ 2\n",
    "            theta = (data[i]+data[i+1])/2\n",
    "    \n",
    "#             init a list that store index that less than theta\n",
    "            index_less_theta = []\n",
    "    \n",
    "#             init a list that store value that less than theta\n",
    "            value_less_theta = []\n",
    "    \n",
    "#             init a list that store index that greater than theta\n",
    "            index_great_theta = []\n",
    "    \n",
    "#             init a list that store value that less than theta\n",
    "            value_great_theta = []\n",
    "    \n",
    "#             init the counter to 0\n",
    "            counter = 0\n",
    "    \n",
    "#             for each index and value in attri_list\n",
    "            for c,j in enumerate(attri_list):\n",
    "#                 if value less or equal than the current theta:\n",
    "                if (float(j[0])<=theta):\n",
    "#                     update the \"less\" list of index and value\n",
    "                    index_less_theta.append(j[1])\n",
    "                    value_less_theta.append(c)\n",
    "                else:\n",
    "#                     update the \"greater\" list of index and value\n",
    "                    index_great_theta.append(j[1])\n",
    "                    value_great_theta.append(c)\n",
    "\n",
    "#             calculate the entropy of the \"less\" list\n",
    "            etr_less = entropy(index_less_theta)\n",
    "            \n",
    "#             calculate the entropy of the \"greater\" list\n",
    "            etr_great = entropy(index_great_theta)\n",
    "            \n",
    "#             calculate the info gain using the formular.\n",
    "            gain = etr - (len(value_less_theta)/len(data))*etr_less - (len(value_great_theta)/len(data))*etr_great\n",
    "#             if current info gain > max info gan\n",
    "            if (gain > max_info_gain):\n",
    "#                 update the info gain, \n",
    "#                     the theta, the best index list of right, \n",
    "#                         the best index list of left and class_labels_list_after_split\n",
    "                max_info_gain = gain\n",
    "                best_l = value_less_theta\n",
    "                best_r = value_great_theta\n",
    "                split_labels = index_less_theta + index_great_theta\n",
    "#         return the max info gain, theata,the best left list,the best right list and class label after split\n",
    "        return max_info_gain, theta, best_l, best_r, split_labels\n",
    "\n",
    "    #method to select the best feature out of all the features.\n",
    "    def best_feature(self,dict_rep):\n",
    "#         set key value to none\n",
    "        key_value = None\n",
    "    \n",
    "#         set best info gain to -1\n",
    "        best_gain = -1\n",
    "    \n",
    "#         set best theta to 0\n",
    "        best_theta = 0\n",
    "    \n",
    "#         set best left list to empty\n",
    "        best_l = []\n",
    "    \n",
    "#         set best right list to empty\n",
    "        best_r = []\n",
    "    \n",
    "#         set best class labels after split to empty\n",
    "        best_split = []\n",
    "    \n",
    "#         init a result list\n",
    "        result = []\n",
    "    \n",
    "#         for each key in dict_rep:\n",
    "        for ke in dict_rep.keys():\n",
    "#             using cal_best_theta_value() and store all returned values\n",
    "            info_gain, theta, index_l, index_r, split_labels = self.cal_best_theta_value(ke,dict_rep[ke])\n",
    "#             if info gian is greater than best info gain:\n",
    "            if (info_gain > best_gain):\n",
    "        \n",
    "#                 update info gain, theth, key value,\n",
    "#                     left list, right list, class labels after split\n",
    "                best_gain = info_gain\n",
    "                best_theta = theta\n",
    "                best_l = index_l\n",
    "                best_r = index_r\n",
    "                best_split = split_labels\n",
    "                key_value = ke\n",
    "\n",
    "#         append the key value to the retrun list\n",
    "        result.append(key_value)\n",
    "    \n",
    "#         append the theta value to the retrun list\n",
    "        result.append(best_theta)\n",
    "    \n",
    "#         append the left list to the retrun list\n",
    "        result.append(best_l)\n",
    "    \n",
    "#         append the right list to the retrun list\n",
    "        result.append(best_r)\n",
    "    \n",
    "#         append the class labels to the retrun list\n",
    "        result.append(best_split)\n",
    "    \n",
    "#         return the list.\n",
    "        return result\n",
    "\n",
    "    def get_remainder_dict(self,dict_of_everything,index_split):\n",
    "        global fea_list\n",
    "#         init a split dict\n",
    "        split = {}\n",
    "    \n",
    "#         for each key \"ke\" in dict_of_everything:\n",
    "        for ke in dict_of_everything.keys():\n",
    "#             init a value list\n",
    "            value = []\n",
    "    \n",
    "#             init a modified list\n",
    "            mod = []\n",
    "    \n",
    "#             get the corresponding values of the key\"ke\" \n",
    "            val = dict_of_everything[ke]\n",
    "    \n",
    "#             for each value and its corresponding index of the key\"ke\" \n",
    "            for c,j in enumerate(val):\n",
    "                \n",
    "#                 if index is not in the index_split:\n",
    "                if(c not in index_split):\n",
    "#                     append it to the modified list and value list\n",
    "                    mod.append(j)\n",
    "                    value.append(j[1])\n",
    "\n",
    "#             add this modified list to the dict\n",
    "            split[ke] = mod\n",
    "#         return the splited dict and val list\n",
    "        return split, value\n",
    "\n",
    "    #method to create decision tree\n",
    "    def create_decision_tree(self, dict_of_everything,class_val,eta_min_val):\n",
    "        global fea_list\n",
    "        #if all the class labels are same, then we are set\n",
    "        if len(set(class_val)) == 1:   #based on the return value, the node clas has to be called to create root nodes with no children\n",
    "            #print(\"Leaf node for set class is\",class_val[0],len(class_val))\n",
    "            root_node =  Node(class_val[0], None, None, 0, True)\n",
    "            return root_node\n",
    "        \n",
    "        #if the no class vales are less than threshold, we assign the class with max values as the class label    \n",
    "        elif len(class_val) < eta_min_val:\n",
    "            major_label = self.cal_major_class_values(class_val)\n",
    "            root_node = Node(major_label, None, None, 0, True)\n",
    "            return root_node\n",
    "        \n",
    "#         else:\n",
    "        else:\n",
    "#             using the best_feature to get best feature list\n",
    "            best_features = self.best_feature(dict_of_everything)\n",
    "    \n",
    "#             store the node name, theta,left split, right split and class labes\n",
    "            node = best_features[0]\n",
    "            theta = best_features[1]\n",
    "            left_split = best_features[2]\n",
    "            right_split = best_features[3]\n",
    "            class_labels = best_features[4]\n",
    "\n",
    "#             call get_remainder_dict to get left tree data\n",
    "            left_dict, left_val = self.get_remainder_dict(dict_of_everything,left_split)\n",
    "    \n",
    "#             call get_remainder_dict to get right tree data\n",
    "            right_dict, right_val = self.get_remainder_dict(dict_of_everything,right_split)\n",
    "    \n",
    "#             call create_decision_tree to get left tree based on the left tree data\n",
    "            left_node = self.create_decision_tree(left_dict,left_val,eta_min_val)\n",
    "    \n",
    "#             call create_decision_tree to get right tree based on therightleft tree data\n",
    "            right_node = self.create_decision_tree(right_dict,right_val,eta_min_val)\n",
    "    \n",
    "#             set the root node\n",
    "            root_node = Node(node, left_node, right_node,theta,False)\n",
    "    \n",
    "#             return root node\n",
    "            return root_node\n",
    "            \n",
    "    #fit the decisin tree\n",
    "    def fit(self, dict_of_everything,cl_val,features,eta_min_val):\n",
    "#         set the fea_list the value of features\n",
    "        global fea_list\n",
    "        fea_list = features\n",
    "        \n",
    "#         set the root node using the function create_decision_tree()\n",
    "        root_node = self.create_decision_tree(dict_of_everything, cl_val,eta_min_val)\n",
    "    \n",
    "        return root_node\n",
    "\n",
    "    def classify(self,row,root):\n",
    "#         init the test dict\n",
    "        test = {}\n",
    "    \n",
    "#         add row to the dict\n",
    "        for k,j in enumerate(row):\n",
    "            test[k] = j\n",
    "            \n",
    "#         set the current node to root\n",
    "        cur_node = root\n",
    "    \n",
    "#         while the current node is not leaf:\n",
    "        while not cur_node.leaf:  \n",
    "#             implement the case whether the current shoud go to the left\n",
    "            if float(test[cur_node.root_value]) <= cur_node.theta: # based on the principle used in cal_best_theta_value to determine whic index list to update\n",
    "                cur_node = cur_node.root_left\n",
    "            else:\n",
    "                cur_node = cur_node.root_right\n",
    "        \n",
    "#         return the calss of the current node\n",
    "        return cur_node.root_value\n",
    "        \n",
    "    #method to the labels for the test data\n",
    "    def predict(self, X, root):\n",
    "#         predict using the classify() \n",
    "        predict = []    #gets a list of predicted values form each row of X base on parameters of classify method\n",
    "        for i in X:\n",
    "            pred = self.classify(i, root)\n",
    "            predict.append(pred)\n",
    "\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the predicited accuracy\n",
    "def accuracy_for_predicted_values(test_class_names1,l):\n",
    "#     init true and false count to 0\n",
    "    truec = 0\n",
    "    falsec = 0\n",
    "    \n",
    "#     for each prediction,if predict is correct then, true++ else, false++\n",
    "    for i in range(len(test_class_names1)):\n",
    "        if(test_class_names1[i] == l[i]):\n",
    "            truec += 1\n",
    "        else:\n",
    "            falsec += 1\n",
    "            \n",
    "#     return the acc\n",
    "    acc = truec/len(l)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_arr, eta_min):\n",
    "    eta_min_val = round(eta_min*num_arr.shape[0])\n",
    "    \n",
    "    #randomly shuffle the array so that we can divide the data into test/training\n",
    "    num_arr = random_numpy_array(num_arr)\n",
    "    \n",
    "    #divide data into test labels,test features,training labels, training features\n",
    "    test_f, test_l, train_f, train_l = generate_set(num_arr)\n",
    "    \n",
    "#     init cumulate acc to 0\n",
    "    acc_cum = 0\n",
    "    #ten fold iteration \n",
    "    for i in range(10):\n",
    "        #build a dictionary with class labels and respective features values belonging to that class\n",
    "        dict_att, features = build_dict_of_attributes_with_class_values(train_f[i],train_l[i])\n",
    "        \n",
    "        #instantiate decision tree instance\n",
    "        d_tree = DecisionTree()\n",
    "        \n",
    "        # build the decision tree model.\n",
    "        root_node = d_tree.fit(dict_att,train_l[i],features,eta_min_val)\n",
    "\n",
    "        #predict the class labels for test features\n",
    "        predict = d_tree.predict(test_f[i], root_node)\n",
    "        #calculate the accuracy for the predicted values\n",
    "        acc = accuracy_for_predicted_values(test_l[i],predict)\n",
    "        \n",
    "#         add acc to cumulate acc\n",
    "        acc_cum += acc\n",
    "        print(\"Accuracy is \",acc)\n",
    "    print(\"Accuracy across 10-cross validation for\",eta_min,\"is\",float(acc_cum)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_min_list = [0.05,0.10,0.15,0.20]\n",
    "newfile = \"iris.csv\"\n",
    "num_arr = load_csv(newfile)\n",
    "for i in eta_min_list:\n",
    "    main(num_arr,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_min_list = [0.05,0.10,0.15,0.20,0.25]\n",
    "newfile = \"spambase.csv\"\n",
    "num_arr = load_csv(newfile)\n",
    "for i in eta_min_list:\n",
    "    main(num_arr,i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
